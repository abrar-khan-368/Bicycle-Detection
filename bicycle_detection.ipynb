{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.9.0\n"
     ]
    }
   ],
   "source": [
    "# import the relevant libraries\n",
    "import numpy as np\n",
    "import cv2 # openCV\n",
    "\n",
    "# check the opencv version\n",
    "print(cv2.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_img = cv2.imread('people_bicycles.jpg')\n",
    "cv2.imshow('Bicycles', test_img)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "scalefactor = 1.0/255.0\n",
    "new_size = (416, 416)\n",
    "blob = cv2.dnn.blobFromImage(test_img, scalefactor, new_size, swapRB=True, crop=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['person',\n",
       " 'bicycle',\n",
       " 'car',\n",
       " 'motorbike',\n",
       " 'aeroplane',\n",
       " 'bus',\n",
       " 'train',\n",
       " 'truck',\n",
       " 'boat',\n",
       " 'traffic light',\n",
       " 'fire hydrant',\n",
       " 'stop sign',\n",
       " 'parking meter',\n",
       " 'bench',\n",
       " 'bird',\n",
       " 'cat',\n",
       " 'dog',\n",
       " 'horse',\n",
       " 'sheep',\n",
       " 'cow',\n",
       " 'elephant',\n",
       " 'bear',\n",
       " 'zebra',\n",
       " 'giraffe',\n",
       " 'backpack',\n",
       " 'umbrella',\n",
       " 'handbag',\n",
       " 'tie',\n",
       " 'suitcase',\n",
       " 'frisbee',\n",
       " 'skis',\n",
       " 'snowboard',\n",
       " 'sports ball',\n",
       " 'kite',\n",
       " 'baseball bat',\n",
       " 'baseball glove',\n",
       " 'skateboard',\n",
       " 'surfboard',\n",
       " 'tennis racket',\n",
       " 'bottle',\n",
       " 'wine glass',\n",
       " 'cup',\n",
       " 'fork',\n",
       " 'knife',\n",
       " 'spoon',\n",
       " 'bowl',\n",
       " 'banana',\n",
       " 'apple',\n",
       " 'sandwich',\n",
       " 'orange',\n",
       " 'broccoli',\n",
       " 'carrot',\n",
       " 'hot dog',\n",
       " 'pizza',\n",
       " 'donut',\n",
       " 'cake',\n",
       " 'chair',\n",
       " 'sofa',\n",
       " 'pottedplant',\n",
       " 'bed',\n",
       " 'diningtable',\n",
       " 'toilet',\n",
       " 'tvmonitor',\n",
       " 'laptop',\n",
       " 'mouse',\n",
       " 'remote',\n",
       " 'keyboard',\n",
       " 'cell phone',\n",
       " 'microwave',\n",
       " 'oven',\n",
       " 'toaster',\n",
       " 'sink',\n",
       " 'refrigerator',\n",
       " 'book',\n",
       " 'clock',\n",
       " 'vase',\n",
       " 'scissors',\n",
       " 'teddy bear',\n",
       " 'hair drier',\n",
       " 'toothbrush']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_labels_path = \"model/coco.names\"\n",
    "class_labels = open(class_labels_path).read().strip().split(\"\\n\")\n",
    "class_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# declare repeating bounding box colors for each class \n",
    "# 1st: create a list colors as an RGB string array\n",
    "# Example: Red, Green, Blue, Yellow, Magenda\n",
    "class_colors = [\"255,0,0\",\"0,255,0\",\"0,0,255\",\"255,255,0\",\"255,0, 255\"]\n",
    " \n",
    "#2nd: split the array on comma-separated strings and for change each string type to integer\n",
    "class_colors = [np.array(every_color.split(\",\")).astype(\"int\") for every_color in class_colors]\n",
    " \n",
    "#3rd: convert the array or arrays to a numpy array\n",
    "class_colors = np.array(class_colors)\n",
    " \n",
    "#4th: tile this to get 80 class colors, i.e. as many as the classes(16 rows of 5cols each). \n",
    "# If you want unique colors for each class you may randomize the color generation or set them manually\n",
    "class_colors = np.tile(class_colors,(16,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;2;255;0;0mclass0 \u001b[38;2;255;255;255m\u001b[38;2;0;255;0mclass1 \u001b[38;2;255;255;255m\u001b[38;2;0;0;255mclass2 \u001b[38;2;255;255;255m\u001b[38;2;255;255;0mclass3 \u001b[38;2;255;255;255m\u001b[38;2;255;0;255mclass4 \u001b[38;2;255;255;255m\n",
      "\u001b[38;2;255;0;0mclass5 \u001b[38;2;255;255;255m\u001b[38;2;0;255;0mclass6 \u001b[38;2;255;255;255m\u001b[38;2;0;0;255mclass7 \u001b[38;2;255;255;255m\u001b[38;2;255;255;0mclass8 \u001b[38;2;255;255;255m\u001b[38;2;255;0;255mclass9 \u001b[38;2;255;255;255m\n",
      "\u001b[38;2;255;0;0mclass10 \u001b[38;2;255;255;255m\u001b[38;2;0;255;0mclass11 \u001b[38;2;255;255;255m\u001b[38;2;0;0;255mclass12 \u001b[38;2;255;255;255m\u001b[38;2;255;255;0mclass13 \u001b[38;2;255;255;255m\u001b[38;2;255;0;255mclass14 \u001b[38;2;255;255;255m\n",
      "\u001b[38;2;255;0;0mclass15 \u001b[38;2;255;255;255m\u001b[38;2;0;255;0mclass16 \u001b[38;2;255;255;255m\u001b[38;2;0;0;255mclass17 \u001b[38;2;255;255;255m\u001b[38;2;255;255;0mclass18 \u001b[38;2;255;255;255m\u001b[38;2;255;0;255mclass19 \u001b[38;2;255;255;255m\n",
      "\u001b[38;2;255;0;0mclass20 \u001b[38;2;255;255;255m\u001b[38;2;0;255;0mclass21 \u001b[38;2;255;255;255m\u001b[38;2;0;0;255mclass22 \u001b[38;2;255;255;255m\u001b[38;2;255;255;0mclass23 \u001b[38;2;255;255;255m\u001b[38;2;255;0;255mclass24 \u001b[38;2;255;255;255m\n",
      "\u001b[38;2;255;0;0mclass25 \u001b[38;2;255;255;255m\u001b[38;2;0;255;0mclass26 \u001b[38;2;255;255;255m\u001b[38;2;0;0;255mclass27 \u001b[38;2;255;255;255m\u001b[38;2;255;255;0mclass28 \u001b[38;2;255;255;255m\u001b[38;2;255;0;255mclass29 \u001b[38;2;255;255;255m\n",
      "\u001b[38;2;255;0;0mclass30 \u001b[38;2;255;255;255m\u001b[38;2;0;255;0mclass31 \u001b[38;2;255;255;255m\u001b[38;2;0;0;255mclass32 \u001b[38;2;255;255;255m\u001b[38;2;255;255;0mclass33 \u001b[38;2;255;255;255m\u001b[38;2;255;0;255mclass34 \u001b[38;2;255;255;255m\n",
      "\u001b[38;2;255;0;0mclass35 \u001b[38;2;255;255;255m\u001b[38;2;0;255;0mclass36 \u001b[38;2;255;255;255m\u001b[38;2;0;0;255mclass37 \u001b[38;2;255;255;255m\u001b[38;2;255;255;0mclass38 \u001b[38;2;255;255;255m\u001b[38;2;255;0;255mclass39 \u001b[38;2;255;255;255m\n",
      "\u001b[38;2;255;0;0mclass40 \u001b[38;2;255;255;255m\u001b[38;2;0;255;0mclass41 \u001b[38;2;255;255;255m\u001b[38;2;0;0;255mclass42 \u001b[38;2;255;255;255m\u001b[38;2;255;255;0mclass43 \u001b[38;2;255;255;255m\u001b[38;2;255;0;255mclass44 \u001b[38;2;255;255;255m\n",
      "\u001b[38;2;255;0;0mclass45 \u001b[38;2;255;255;255m\u001b[38;2;0;255;0mclass46 \u001b[38;2;255;255;255m\u001b[38;2;0;0;255mclass47 \u001b[38;2;255;255;255m\u001b[38;2;255;255;0mclass48 \u001b[38;2;255;255;255m\u001b[38;2;255;0;255mclass49 \u001b[38;2;255;255;255m\n",
      "\u001b[38;2;255;0;0mclass50 \u001b[38;2;255;255;255m\u001b[38;2;0;255;0mclass51 \u001b[38;2;255;255;255m\u001b[38;2;0;0;255mclass52 \u001b[38;2;255;255;255m\u001b[38;2;255;255;0mclass53 \u001b[38;2;255;255;255m\u001b[38;2;255;0;255mclass54 \u001b[38;2;255;255;255m\n",
      "\u001b[38;2;255;0;0mclass55 \u001b[38;2;255;255;255m\u001b[38;2;0;255;0mclass56 \u001b[38;2;255;255;255m\u001b[38;2;0;0;255mclass57 \u001b[38;2;255;255;255m\u001b[38;2;255;255;0mclass58 \u001b[38;2;255;255;255m\u001b[38;2;255;0;255mclass59 \u001b[38;2;255;255;255m\n",
      "\u001b[38;2;255;0;0mclass60 \u001b[38;2;255;255;255m\u001b[38;2;0;255;0mclass61 \u001b[38;2;255;255;255m\u001b[38;2;0;0;255mclass62 \u001b[38;2;255;255;255m\u001b[38;2;255;255;0mclass63 \u001b[38;2;255;255;255m\u001b[38;2;255;0;255mclass64 \u001b[38;2;255;255;255m\n",
      "\u001b[38;2;255;0;0mclass65 \u001b[38;2;255;255;255m\u001b[38;2;0;255;0mclass66 \u001b[38;2;255;255;255m\u001b[38;2;0;0;255mclass67 \u001b[38;2;255;255;255m\u001b[38;2;255;255;0mclass68 \u001b[38;2;255;255;255m\u001b[38;2;255;0;255mclass69 \u001b[38;2;255;255;255m\n",
      "\u001b[38;2;255;0;0mclass70 \u001b[38;2;255;255;255m\u001b[38;2;0;255;0mclass71 \u001b[38;2;255;255;255m\u001b[38;2;0;0;255mclass72 \u001b[38;2;255;255;255m\u001b[38;2;255;255;0mclass73 \u001b[38;2;255;255;255m\u001b[38;2;255;0;255mclass74 \u001b[38;2;255;255;255m\n",
      "\u001b[38;2;255;0;0mclass75 \u001b[38;2;255;255;255m\u001b[38;2;0;255;0mclass76 \u001b[38;2;255;255;255m\u001b[38;2;0;0;255mclass77 \u001b[38;2;255;255;255m\u001b[38;2;255;255;0mclass78 \u001b[38;2;255;255;255m\u001b[38;2;255;0;255mclass79 \u001b[38;2;255;255;255m\n"
     ]
    }
   ],
   "source": [
    "def colored(r, g, b, text):\n",
    "    return \"\\033[38;2;{};{};{}m{} \\033[38;2;255;255;255m\".format(r, g, b, text)\n",
    " \n",
    "for i in range(16):\n",
    "  line = \"\"\n",
    "  for j in range(5):\n",
    "    class_id = i*5 + j\n",
    "    class_id_str = str(class_id)\n",
    "    text = \"class\" + class_id_str\n",
    "    colored_text = colored(class_colors[class_id][0], class_colors[class_id][1], class_colors[class_id][2], text)\n",
    "    line += colored_text\n",
    "  print(line)\n",
    "\n",
    "# or select the colors randomly\n",
    "class_colors = np.random.randint(0, 255, size=(len(class_labels), 3), dtype=\"uint8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = cv2.dnn.readNetFromDarknet('model/yolov4.cfg', 'model/yolov4.weights')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of network components: 379\n",
      "('conv_0', 'bn_0', 'mish_1', 'conv_1', 'bn_1', 'mish_2', 'conv_2', 'bn_2', 'mish_3', 'identity_3', 'conv_4', 'bn_4', 'mish_5', 'conv_5', 'bn_5', 'mish_6', 'conv_6', 'bn_6', 'mish_7', 'shortcut_7', 'conv_8', 'bn_8', 'mish_9', 'concat_9', 'conv_10', 'bn_10', 'mish_11', 'conv_11', 'bn_11', 'mish_12', 'conv_12', 'bn_12', 'mish_13', 'identity_13', 'conv_14', 'bn_14', 'mish_15', 'conv_15', 'bn_15', 'mish_16', 'conv_16', 'bn_16', 'mish_17', 'shortcut_17', 'conv_18', 'bn_18', 'mish_19', 'conv_19', 'bn_19', 'mish_20', 'shortcut_20', 'conv_21', 'bn_21', 'mish_22', 'concat_22', 'conv_23', 'bn_23', 'mish_24', 'conv_24', 'bn_24', 'mish_25', 'conv_25', 'bn_25', 'mish_26', 'identity_26', 'conv_27', 'bn_27', 'mish_28', 'conv_28', 'bn_28', 'mish_29', 'conv_29', 'bn_29', 'mish_30', 'shortcut_30', 'conv_31', 'bn_31', 'mish_32', 'conv_32', 'bn_32', 'mish_33', 'shortcut_33', 'conv_34', 'bn_34', 'mish_35', 'conv_35', 'bn_35', 'mish_36', 'shortcut_36', 'conv_37', 'bn_37', 'mish_38', 'conv_38', 'bn_38', 'mish_39', 'shortcut_39', 'conv_40', 'bn_40', 'mish_41', 'conv_41', 'bn_41', 'mish_42', 'shortcut_42', 'conv_43', 'bn_43', 'mish_44', 'conv_44', 'bn_44', 'mish_45', 'shortcut_45', 'conv_46', 'bn_46', 'mish_47', 'conv_47', 'bn_47', 'mish_48', 'shortcut_48', 'conv_49', 'bn_49', 'mish_50', 'conv_50', 'bn_50', 'mish_51', 'shortcut_51', 'conv_52', 'bn_52', 'mish_53', 'concat_53', 'conv_54', 'bn_54', 'mish_55', 'conv_55', 'bn_55', 'mish_56', 'conv_56', 'bn_56', 'mish_57', 'identity_57', 'conv_58', 'bn_58', 'mish_59', 'conv_59', 'bn_59', 'mish_60', 'conv_60', 'bn_60', 'mish_61', 'shortcut_61', 'conv_62', 'bn_62', 'mish_63', 'conv_63', 'bn_63', 'mish_64', 'shortcut_64', 'conv_65', 'bn_65', 'mish_66', 'conv_66', 'bn_66', 'mish_67', 'shortcut_67', 'conv_68', 'bn_68', 'mish_69', 'conv_69', 'bn_69', 'mish_70', 'shortcut_70', 'conv_71', 'bn_71', 'mish_72', 'conv_72', 'bn_72', 'mish_73', 'shortcut_73', 'conv_74', 'bn_74', 'mish_75', 'conv_75', 'bn_75', 'mish_76', 'shortcut_76', 'conv_77', 'bn_77', 'mish_78', 'conv_78', 'bn_78', 'mish_79', 'shortcut_79', 'conv_80', 'bn_80', 'mish_81', 'conv_81', 'bn_81', 'mish_82', 'shortcut_82', 'conv_83', 'bn_83', 'mish_84', 'concat_84', 'conv_85', 'bn_85', 'mish_86', 'conv_86', 'bn_86', 'mish_87', 'conv_87', 'bn_87', 'mish_88', 'identity_88', 'conv_89', 'bn_89', 'mish_90', 'conv_90', 'bn_90', 'mish_91', 'conv_91', 'bn_91', 'mish_92', 'shortcut_92', 'conv_93', 'bn_93', 'mish_94', 'conv_94', 'bn_94', 'mish_95', 'shortcut_95', 'conv_96', 'bn_96', 'mish_97', 'conv_97', 'bn_97', 'mish_98', 'shortcut_98', 'conv_99', 'bn_99', 'mish_100', 'conv_100', 'bn_100', 'mish_101', 'shortcut_101', 'conv_102', 'bn_102', 'mish_103', 'concat_103', 'conv_104', 'bn_104', 'mish_105', 'conv_105', 'bn_105', 'leaky_106', 'conv_106', 'bn_106', 'leaky_107', 'conv_107', 'bn_107', 'leaky_108', 'pool_108', 'identity_109', 'pool_110', 'identity_111', 'pool_112', 'concat_113', 'conv_114', 'bn_114', 'leaky_115', 'conv_115', 'bn_115', 'leaky_116', 'conv_116', 'bn_116', 'leaky_117', 'conv_117', 'bn_117', 'leaky_118', 'upsample_118', 'identity_119', 'conv_120', 'bn_120', 'leaky_121', 'concat_121', 'conv_122', 'bn_122', 'leaky_123', 'conv_123', 'bn_123', 'leaky_124', 'conv_124', 'bn_124', 'leaky_125', 'conv_125', 'bn_125', 'leaky_126', 'conv_126', 'bn_126', 'leaky_127', 'conv_127', 'bn_127', 'leaky_128', 'upsample_128', 'identity_129', 'conv_130', 'bn_130', 'leaky_131', 'concat_131', 'conv_132', 'bn_132', 'leaky_133', 'conv_133', 'bn_133', 'leaky_134', 'conv_134', 'bn_134', 'leaky_135', 'conv_135', 'bn_135', 'leaky_136', 'conv_136', 'bn_136', 'leaky_137', 'conv_137', 'bn_137', 'leaky_138', 'conv_138', 'permute_139', 'yolo_139', 'identity_140', 'conv_141', 'bn_141', 'leaky_142', 'concat_142', 'conv_143', 'bn_143', 'leaky_144', 'conv_144', 'bn_144', 'leaky_145', 'conv_145', 'bn_145', 'leaky_146', 'conv_146', 'bn_146', 'leaky_147', 'conv_147', 'bn_147', 'leaky_148', 'conv_148', 'bn_148', 'leaky_149', 'conv_149', 'permute_150', 'yolo_150', 'identity_151', 'conv_152', 'bn_152', 'leaky_153', 'concat_153', 'conv_154', 'bn_154', 'leaky_155', 'conv_155', 'bn_155', 'leaky_156', 'conv_156', 'bn_156', 'leaky_157', 'conv_157', 'bn_157', 'leaky_158', 'conv_158', 'bn_158', 'leaky_159', 'conv_159', 'bn_159', 'leaky_160', 'conv_160', 'permute_161', 'yolo_161')\n",
      "['yolo_139', 'yolo_150', 'yolo_161']\n"
     ]
    }
   ],
   "source": [
    "layers = network.getLayerNames()\n",
    "print(\"number of network components: \" + str(len(layers)))\n",
    "print(layers)\n",
    "yolo_layers = [layers[i - 1] for i in network.getUnconnectedOutLayers()]\n",
    "print(yolo_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of sets of detections: 3\n"
     ]
    }
   ],
   "source": [
    "# input pre-processed blob into the model\n",
    "network.setInput(blob)\n",
    " \n",
    "# compute the forward pass for the input, storing the results per output layer in a list\n",
    "obj_detections_in_layers = network.forward(yolo_layers)\n",
    " \n",
    "# verify the number of sets of detections\n",
    "print(\"number of sets of detections: \" + str(len(obj_detections_in_layers)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def object_detection_analysis(test_image, obj_detections_in_layers, confidence_threshold): \n",
    " \n",
    "  # get the image dimensions  \n",
    "  img_height = test_img.shape[0]\n",
    "  img_width = test_img.shape[1]\n",
    " \n",
    "  result = test_image.copy()\n",
    "  \n",
    "  # loop over each output layer \n",
    "  for object_detections_in_single_layer in obj_detections_in_layers:\n",
    "    # loop over the detections in each layer\n",
    "      for object_detection in object_detections_in_single_layer:  \n",
    "        # obj_detection[1]: bbox center pt_x\n",
    "        # obj_detection[2]: bbox center pt_y\n",
    "        # obj_detection[3]: bbox width\n",
    "        # obj_detection[4]: bbox height\n",
    "        # obj_detection[5]: confidence scores for all detections within the bbox \n",
    " \n",
    "        # get the confidence scores of all objects detected with the bounding box\n",
    "        prediction_scores = object_detection[5:]\n",
    "        # consider the highest score being associated with the winning class\n",
    "        # get the class ID from the index of the highest score \n",
    "        predicted_class_id = np.argmax(prediction_scores)\n",
    "        # get the prediction confidence\n",
    "        prediction_confidence = prediction_scores[predicted_class_id]\n",
    "    \n",
    "        # consider object detections with confidence score higher than threshold\n",
    "        if prediction_confidence > confidence_threshold:\n",
    "            # get the predicted label\n",
    "            predicted_class_label = class_labels[predicted_class_id]\n",
    "            # compute the bounding box coordinates scaled for the input image \n",
    "            # scaling is a multiplication of the float coordinate with the appropriate  image dimension\n",
    "            bounding_box = object_detection[0:4] * np.array([img_width, img_height, img_width, img_height])\n",
    "            # get the bounding box centroid (x,y), width and height as integers\n",
    "            (box_center_x_pt, box_center_y_pt, box_width, box_height) = bounding_box.astype(\"int\")\n",
    "            # to get the start x and y coordinates we to subtract from the centroid half the width and half the height respectively \n",
    "            # for even values of width and height of bboxes adjacent to the  image border\n",
    "            #  this may generate a -1 which is prevented by the max() operator below  \n",
    "            start_x_pt = max(0, int(box_center_x_pt - (box_width / 2)))\n",
    "            start_y_pt = max(0, int(box_center_y_pt - (box_height / 2)))\n",
    "            end_x_pt = start_x_pt + box_width\n",
    "            end_y_pt = start_y_pt + box_height\n",
    "            \n",
    "            # get a random mask color from the numpy array of colors\n",
    "            box_color = class_colors[predicted_class_id]\n",
    "            \n",
    "            # convert the color numpy array as a list and apply to text and box\n",
    "            box_color = [int(c) for c in box_color]\n",
    "            \n",
    "            # print the prediction in console\n",
    "            predicted_class_label = \"{}: {:.2f}%\".format(predicted_class_label, prediction_confidence * 100)\n",
    "            print(\"predicted object {}\".format(predicted_class_label))\n",
    "            \n",
    "            # draw the rectangle and text in the image\n",
    "            cv2.rectangle(result, (start_x_pt, start_y_pt), (end_x_pt, end_y_pt), box_color, 1)\n",
    "            cv2.putText(result, predicted_class_label, (start_x_pt, start_y_pt-5), cv2.FONT_HERSHEY_SIMPLEX, 0.5, box_color, 1)\n",
    "  return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted object traffic light: 56.24%\n",
      "predicted object traffic light: 56.25%\n",
      "predicted object traffic light: 52.78%\n",
      "predicted object traffic light: 33.06%\n",
      "predicted object traffic light: 31.07%\n",
      "predicted object traffic light: 30.06%\n",
      "predicted object traffic light: 34.51%\n",
      "predicted object traffic light: 34.09%\n",
      "predicted object traffic light: 27.52%\n",
      "predicted object traffic light: 23.15%\n",
      "predicted object traffic light: 22.80%\n",
      "predicted object stop sign: 43.93%\n",
      "predicted object stop sign: 43.61%\n",
      "predicted object stop sign: 41.78%\n",
      "predicted object person: 24.01%\n",
      "predicted object person: 23.02%\n",
      "predicted object car: 20.48%\n",
      "predicted object car: 20.05%\n",
      "predicted object person: 42.55%\n",
      "predicted object person: 39.85%\n",
      "predicted object person: 21.27%\n",
      "predicted object person: 20.06%\n",
      "predicted object person: 38.13%\n",
      "predicted object person: 36.41%\n",
      "predicted object car: 35.70%\n",
      "predicted object car: 36.26%\n",
      "predicted object backpack: 26.10%\n",
      "predicted object backpack: 27.40%\n",
      "predicted object car: 75.51%\n",
      "predicted object car: 75.89%\n",
      "predicted object person: 23.32%\n",
      "predicted object person: 76.72%\n",
      "predicted object person: 76.09%\n",
      "predicted object person: 45.59%\n",
      "predicted object person: 44.32%\n",
      "predicted object person: 21.37%\n",
      "predicted object person: 40.40%\n",
      "predicted object person: 88.53%\n",
      "predicted object person: 88.84%\n",
      "predicted object person: 57.29%\n",
      "predicted object person: 54.69%\n",
      "predicted object person: 48.61%\n",
      "predicted object person: 78.84%\n",
      "predicted object car: 22.03%\n",
      "predicted object bicycle: 27.00%\n",
      "predicted object bicycle: 28.13%\n",
      "predicted object bicycle: 25.33%\n",
      "predicted object bicycle: 65.29%\n",
      "predicted object bicycle: 68.52%\n",
      "predicted object bicycle: 60.08%\n",
      "predicted object bicycle: 22.42%\n",
      "predicted object bicycle: 30.99%\n",
      "predicted object bicycle: 30.90%\n",
      "predicted object bicycle: 46.78%\n",
      "predicted object bicycle: 57.65%\n",
      "predicted object bicycle: 55.67%\n",
      "predicted object bicycle: 57.56%\n",
      "predicted object bicycle: 62.42%\n",
      "predicted object bicycle: 58.94%\n",
      "predicted object bicycle: 44.14%\n",
      "predicted object bicycle: 43.84%\n",
      "predicted object bicycle: 38.17%\n",
      "predicted object bicycle: 24.36%\n",
      "predicted object bicycle: 24.55%\n",
      "predicted object bicycle: 42.88%\n",
      "predicted object bicycle: 42.19%\n",
      "predicted object bicycle: 42.08%\n",
      "predicted object bicycle: 42.99%\n",
      "predicted object bicycle: 23.26%\n",
      "predicted object bicycle: 51.90%\n",
      "predicted object bicycle: 48.69%\n",
      "predicted object bicycle: 32.84%\n",
      "predicted object bicycle: 29.92%\n",
      "predicted object bicycle: 71.82%\n",
      "predicted object bicycle: 72.64%\n",
      "predicted object bicycle: 36.69%\n",
      "predicted object bicycle: 33.54%\n",
      "predicted object person: 20.85%\n",
      "predicted object person: 50.63%\n",
      "predicted object person: 47.67%\n",
      "predicted object car: 23.24%\n",
      "predicted object person: 30.55%\n",
      "predicted object person: 20.13%\n",
      "predicted object car: 50.93%\n",
      "predicted object car: 44.98%\n",
      "predicted object person: 51.23%\n",
      "predicted object person: 50.29%\n",
      "predicted object backpack: 45.09%\n",
      "predicted object backpack: 48.98%\n",
      "predicted object backpack: 39.21%\n",
      "predicted object car: 82.24%\n",
      "predicted object car: 79.32%\n",
      "predicted object person: 61.29%\n",
      "predicted object person: 59.29%\n",
      "predicted object person: 62.80%\n",
      "predicted object person: 92.78%\n",
      "predicted object person: 89.45%\n",
      "predicted object person: 81.44%\n",
      "predicted object person: 70.21%\n",
      "predicted object person: 48.96%\n",
      "predicted object car: 43.78%\n",
      "predicted object car: 43.41%\n",
      "predicted object car: 42.45%\n",
      "predicted object car: 63.95%\n",
      "predicted object car: 64.30%\n",
      "predicted object car: 63.98%\n",
      "predicted object car: 28.50%\n",
      "predicted object car: 30.67%\n",
      "predicted object car: 93.75%\n",
      "predicted object car: 93.73%\n",
      "predicted object car: 94.08%\n",
      "predicted object car: 53.66%\n",
      "predicted object car: 53.38%\n",
      "predicted object car: 55.61%\n",
      "predicted object person: 98.20%\n",
      "predicted object person: 98.10%\n",
      "predicted object person: 97.80%\n",
      "predicted object person: 97.04%\n",
      "predicted object person: 96.75%\n",
      "predicted object person: 96.84%\n",
      "predicted object bicycle: 23.27%\n",
      "predicted object bicycle: 42.87%\n",
      "predicted object person: 95.11%\n",
      "predicted object person: 94.75%\n",
      "predicted object person: 94.40%\n",
      "predicted object bicycle: 36.38%\n",
      "predicted object bicycle: 25.33%\n",
      "predicted object bicycle: 46.50%\n",
      "predicted object person: 76.98%\n",
      "predicted object bicycle: 88.47%\n",
      "predicted object bicycle: 64.07%\n",
      "predicted object person: 98.02%\n",
      "predicted object person: 92.72%\n",
      "predicted object bicycle: 85.44%\n",
      "predicted object bicycle: 83.98%\n",
      "predicted object bicycle: 77.90%\n",
      "predicted object person: 69.53%\n",
      "predicted object person: 96.69%\n",
      "predicted object bicycle: 75.94%\n",
      "predicted object bicycle: 69.45%\n",
      "predicted object bicycle: 59.99%\n",
      "predicted object person: 47.47%\n",
      "predicted object bicycle: 22.45%\n",
      "predicted object bicycle: 20.28%\n",
      "predicted object bicycle: 34.55%\n",
      "predicted object bicycle: 41.95%\n",
      "predicted object bicycle: 30.03%\n",
      "predicted object bicycle: 28.83%\n",
      "predicted object bicycle: 41.37%\n",
      "predicted object bicycle: 58.53%\n",
      "predicted object bicycle: 60.09%\n",
      "predicted object bicycle: 78.46%\n",
      "predicted object bicycle: 92.29%\n",
      "predicted object bicycle: 92.90%\n",
      "predicted object bicycle: 53.34%\n",
      "predicted object bicycle: 84.25%\n",
      "predicted object bicycle: 83.41%\n",
      "predicted object backpack: 29.05%\n",
      "predicted object person: 21.98%\n",
      "predicted object car: 38.11%\n",
      "predicted object car: 73.61%\n",
      "predicted object car: 49.64%\n",
      "predicted object car: 31.23%\n",
      "predicted object car: 25.04%\n",
      "predicted object car: 88.75%\n",
      "predicted object car: 37.44%\n",
      "predicted object person: 99.03%\n",
      "predicted object person: 89.97%\n",
      "predicted object person: 97.48%\n",
      "predicted object person: 97.85%\n",
      "predicted object person: 91.86%\n",
      "predicted object person: 92.21%\n",
      "predicted object bicycle: 35.36%\n",
      "predicted object person: 83.00%\n",
      "predicted object person: 85.36%\n",
      "predicted object person: 98.98%\n",
      "predicted object person: 98.88%\n",
      "predicted object bicycle: 26.79%\n",
      "predicted object bicycle: 42.98%\n",
      "predicted object bicycle: 39.53%\n",
      "predicted object bicycle: 70.19%\n",
      "predicted object bicycle: 69.56%\n",
      "predicted object bicycle: 38.80%\n",
      "predicted object bicycle: 30.52%\n",
      "predicted object bicycle: 71.95%\n",
      "predicted object bicycle: 67.22%\n",
      "predicted object bicycle: 21.31%\n",
      "predicted object bicycle: 21.06%\n",
      "predicted object bicycle: 97.53%\n",
      "predicted object bicycle: 96.64%\n"
     ]
    }
   ],
   "source": [
    "confidence_threshold = 0.2\n",
    "result_raw = object_detection_analysis(test_img, obj_detections_in_layers, confidence_threshold)\n",
    "cv2.imshow('Predicated Bicycles', result_raw)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_ids_list = []\n",
    "boxes_list = []\n",
    "confidences_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def object_detection_attributes(test_image, obj_detections_in_layers, confidence_threshold):\n",
    "  # get the image dimensions  \n",
    "  img_height = test_img.shape[0]\n",
    "  img_width = test_img.shape[1]\n",
    "  \n",
    "  # loop over each output layer \n",
    "  for object_detections_in_single_layer in obj_detections_in_layers:\n",
    "    # loop over the detections in each layer\n",
    "    for object_detection in object_detections_in_single_layer:  \n",
    "      # get the confidence scores of all objects detected with the bounding box\n",
    "      prediction_scores = object_detection[5:]\n",
    "      # consider the highest score being associated with the winning class\n",
    "      # get the class ID from the index of the highest score \n",
    "      predicted_class_id = np.argmax(prediction_scores)\n",
    "      # get the prediction confidence\n",
    "      prediction_confidence = prediction_scores[predicted_class_id]\n",
    "      \n",
    "      # consider object detections with confidence score higher than threshold\n",
    "      if prediction_confidence > confidence_threshold:\n",
    "        # get the predicted label\n",
    "        predicted_class_label = class_labels[predicted_class_id]\n",
    "        # compute the bounding box coordinates scaled for the input image\n",
    "        bounding_box = object_detection[0:4] * np.array([img_width, img_height, img_width, img_height])\n",
    "        (box_center_x_pt, box_center_y_pt, box_width, box_height) = bounding_box.astype(\"int\")\n",
    "        start_x_pt = max(0, int(box_center_x_pt - (box_width / 2)))\n",
    "        start_y_pt = max(0, int(box_center_y_pt - (box_height / 2)))\n",
    "        \n",
    "        # update the 3 lists for nms processing\n",
    "        # - confidence is needed as a float \n",
    "        # - the bbox info has the openCV Rect format\n",
    "        class_ids_list.append(predicted_class_id)\n",
    "        confidences_list.append(float(prediction_confidence))\n",
    "        boxes_list.append([int(start_x_pt), int(start_y_pt), int(box_width), int(box_height)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_threshold = 0.5\n",
    "object_detection_attributes(test_img, obj_detections_in_layers, score_threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "nms_threshold = 0.4\n",
    "winner_ids = cv2.dnn.NMSBoxes(boxes_list, confidences_list, score_threshold, nms_threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted object person: 99.03%\n",
      "predicted object person: 98.98%\n",
      "predicted object person: 98.02%\n",
      "predicted object person: 97.04%\n",
      "predicted object car: 94.08%\n",
      "predicted object person: 92.78%\n",
      "predicted object bicycle: 88.47%\n",
      "predicted object bicycle: 85.44%\n",
      "predicted object car: 82.24%\n",
      "predicted object person: 81.44%\n",
      "predicted object car: 73.61%\n",
      "predicted object bicycle: 71.95%\n",
      "predicted object bicycle: 68.52%\n",
      "predicted object bicycle: 62.42%\n",
      "predicted object bicycle: 57.65%\n",
      "predicted object traffic light: 56.25%\n",
      "predicted object bicycle: 53.34%\n",
      "predicted object person: 51.23%\n"
     ]
    }
   ],
   "source": [
    "for winner_id in winner_ids:\n",
    "    max_class_id = winner_id\n",
    "    box = boxes_list[max_class_id]\n",
    "    start_x_pt = box[0]\n",
    "    start_y_pt = box[1]\n",
    "    box_width = box[2]\n",
    "    box_height = box[3]\n",
    "    \n",
    "    #get the predicted class id and label\n",
    "    predicted_class_id = class_ids_list[max_class_id]\n",
    "    predicted_class_label = class_labels[predicted_class_id]\n",
    "    prediction_confidence = confidences_list[max_class_id]\n",
    " \n",
    "    #obtain the bounding box end coordinates\n",
    "    end_x_pt = start_x_pt + box_width\n",
    "    end_y_pt = start_y_pt + box_height\n",
    "    \n",
    "    #get a random mask color from the numpy array of colors\n",
    "    box_color = class_colors[predicted_class_id]\n",
    "    \n",
    "    #convert the color numpy array as a list and apply to text and box\n",
    "    box_color = [int(c) for c in box_color]\n",
    "    \n",
    "    # print the prediction in console\n",
    "    predicted_class_label = \"{}: {:.2f}%\".format(predicted_class_label, prediction_confidence * 100)\n",
    "    print(\"predicted object {}\".format(predicted_class_label))\n",
    "    \n",
    "    # draw rectangle and text in the image\n",
    "    cv2.rectangle(test_img, (start_x_pt, start_y_pt), (end_x_pt, end_y_pt), box_color, 1)\n",
    "    cv2.putText(test_img, predicted_class_label, (start_x_pt, start_y_pt-5), cv2.FONT_HERSHEY_SIMPLEX, 0.5, box_color, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.imshow('Predicated With NMS', test_img)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
